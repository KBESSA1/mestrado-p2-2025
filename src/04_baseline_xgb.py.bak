import sys
import xgboost as xgb
import argparse, pandas as pd, numpy as np
from pathlib import Path
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import GroupKFold
from xgboost import XGBRegressor

# === xgb monkeypatch begin ===
def _xgb_read_flags(argv):
    p = argparse.ArgumentParser(add_help=False)
    p.add_argument("--n_estimators", type=int)
    p.add_argument("--max_depth", type=int)
    p.add_argument("--eta", type=float)
    p.add_argument("--subsample", type=float)
    p.add_argument("--colsample_bytree", type=float)
    p.add_argument("--reg_lambda", type=float)
    opts, _ = p.parse_known_args(argv)
    return opts

_orig_parse_args_x = argparse.ArgumentParser.parse_args
def _x_parse_args(self, *a, **kw):
    try:
        self.add_argument("--n_estimators", type=int, default=None)
        self.add_argument("--max_depth", type=int, default=None)
        self.add_argument("--eta", type=float, default=None)
        self.add_argument("--subsample", type=float, default=None)
        self.add_argument("--colsample_bytree", type=float, default=None)
        self.add_argument("--reg_lambda", type=float, default=None)
    except Exception:
        pass
    return _orig_parse_args_x(self, *a, **kw)
argparse.ArgumentParser.parse_args = _x_parse_args

try:
    import xgboost as xgb
    _XGB_orig_init = xgb.XGBRegressor.__init__
    def _XGB_init(self, *a, **kw):
        opts = _xgb_read_flags(sys.argv[1:])
        if opts.n_estimators is not None: kw["n_estimators"] = opts.n_estimators
        if opts.max_depth    is not None: kw["max_depth"]    = opts.max_depth
        if opts.eta          is not None: kw["learning_rate"] = opts.eta
        if opts.subsample    is not None: kw["subsample"]    = opts.subsample
        if opts.colsample_bytree is not None: kw["colsample_bytree"] = opts.colsample_bytree
        if opts.reg_lambda   is not None: kw["reg_lambda"]   = opts.reg_lambda
        return _XGB_orig_init(self, *a, **kw)
    xgb.XGBRegressor.__init__ = _XGB_init
except Exception:
    pass
# === xgb monkeypatch end ===

def pick_features(df, target, date_col):
    drop = {target, date_col, "Satellite_Images_Dates", "Latitude", "Longitude", "SampleID"}
    cols = [c for c in df.columns if c not in drop and pd.api.types.is_numeric_dtype(df[c])]
    return cols

def lodo_eval(df, date_col, feats, target):
    dates = pd.to_datetime(df[date_col]).dt.date
    X = df[feats].values
    y = df[target].values
    gkf = GroupKFold(n_splits=len(np.unique(dates)))
    rows = []
    for (train_idx, test_idx), d in zip(gkf.split(X, y, groups=dates), np.unique(dates)):
        Xtr, Xte = X[train_idx], X[test_idx]
        ytr, yte = y[train_idx], y[test_idx]
        mtr = ~np.isnan(Xtr).any(axis=1) & ~np.isnan(ytr)
        mte = ~np.isnan(Xte).any(axis=1) & ~np.isnan(yte)
        Xtr, ytr = Xtr[mtr], ytr[mtr]
        Xte, yte = Xte[mte], yte[mte]
        if len(ytr)==0 or len(yte)==0:
            r2 = rmse = mae = np.nan
        else:
            model = xgb.XGBRegressor(
    n_estimators=args.n_estimators,
    max_depth=args.max_depth,
    learning_rate=args.learning_rate,
    subsample=args.subsample,
    colsample_bytree=args.colsample_bytree,
    reg_lambda=args.reg_lambda,
    reg_alpha=args.reg_alpha,
    n_jobs=-1,
    random_state=42,
    tree_method='hist',
)
            model.set_params(**rparams_xgb)

            model.fit(Xtr, ytr)
            pred = model.predict(Xte)
            r2 = r2_score(yte, pred)
            rmse = mean_squared_error(yte, pred, squared=True) ** 0.5
            mae = mean_absolute_error(yte, pred)
        rows.append({"heldout_date": str(d), "r2": r2, "rmse": rmse, "mae": mae})
    return pd.DataFrame(rows)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True)
    ap.add_argument("--date-col", required=True)
    ap.add_argument("--target-col", required=True)
    ap.add_argument("--out", required=True)
    args = ap.add_argument("--n_estimators", type=int, default=800)
ap.add_argument("--max_depth", type=int, default=4)
ap.add_argument("--learning_rate", type=float, default=0.06)
ap.add_argument("--eta", type=float, default=None)  # alias
ap.add_argument("--subsample", type=float, default=0.7)
ap.add_argument("--colsample_bytree", type=float, default=0.7)
ap.add_argument("--reg_lambda", type=float, default=1.0)
ap.add_argument("--reg_alpha", type=float, default=0.0)

# se --eta foi passado, sobrescreve learning_rate
if ap.get_default("eta") is not None:
    pass  # placeholder para manter bloco válido
ap.parse_args()
if hasattr(args, 'eta') and args.eta is not None:
    args.learning_rate = args.eta

    df = pd.read_csv(args.csv)
    if args.date_col not in df.columns:
        raise SystemExit(f"coluna de data '{args.date_col}' não existe")
    if args.target_col not in df.columns:
        raise SystemExit(f"target '{args.target_col}' não existe")

    feats = pick_features(df, args.target_col, args.date_col)
    res = lodo_eval(df, args.date_col, feats, args.target_col)

    mean_row = {"heldout_date": "__mean__", "r2": res["r2"].mean(),
                "rmse": res["rmse"].mean(), "mae": res["mae"].mean()}
    out = pd.concat([res, pd.DataFrame([mean_row])], ignore_index=True)
    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(args.out, index=False)
    print(f"ok -> {args.out} | feats={len(feats)} | alvo={args.target_col}")

if __name__ == "__main__":
    main()
